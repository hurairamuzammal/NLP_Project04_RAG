{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "2317c1e4",
            "metadata": {},
            "source": [
                "# First cleaning our medicalknowlege base"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "path_helper",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Searching for dataset root from: c:\\Users\\Muhammad Abu Huraira\\Documents\\Assignments and Submissions\\Semester 7\\NLP\\A04\\NLP_Project04_RAG\\mimic-iv-ext-direct-1.0.0\\My_dataset\n",
                        "Dataset root found at: c:\\Users\\Muhammad Abu Huraira\\Documents\\Assignments and Submissions\\Semester 7\\NLP\\A04\\NLP_Project04_RAG\\mimic-iv-ext-direct-1.0.0\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "\n",
                "def get_dataset_paths():\n",
                "    \"\"\"Finds the absolute paths for the dataset folders.\"\"\"\n",
                "    current_dir = os.getcwd()\n",
                "    print(f\"Searching for dataset root from: {current_dir}\")\n",
                "    \n",
                "    # We are looking for 'mimic-iv-ext-direct-1.0.0' which contains 'medicalKnowledgeBase'\n",
                "    # Check current directory and parents\n",
                "    check_path = current_dir\n",
                "    dataset_root = None\n",
                "    \n",
                "    for _ in range(5): # Check up to 5 levels up\n",
                "        # Case 1: We are inside mimic-iv-ext-direct-1.0.0\n",
                "        if os.path.exists(os.path.join(check_path, \"medicalKnowledgeBase\")):\n",
                "            dataset_root = check_path\n",
                "            break\n",
                "            \n",
                "        # Case 2: We are in the parent of mimic-iv-ext-direct-1.0.0\n",
                "        if os.path.exists(os.path.join(check_path, \"mimic-iv-ext-direct-1.0.0\", \"medicalKnowledgeBase\")):\n",
                "            dataset_root = os.path.join(check_path, \"mimic-iv-ext-direct-1.0.0\")\n",
                "            break\n",
                "            \n",
                "        parent = os.path.dirname(check_path)\n",
                "        if parent == check_path:\n",
                "            break\n",
                "        check_path = parent\n",
                "    \n",
                "    if dataset_root:\n",
                "        print(f\"Dataset root found at: {dataset_root}\")\n",
                "        return {\n",
                "            \"root\": dataset_root,\n",
                "            \"kb\": os.path.join(dataset_root, \"medicalKnowledgeBase\", \"Diagnosis_flowchart\"),\n",
                "            \"patient_cases\": os.path.join(dataset_root, \"patient_cases\"),\n",
                "            \"finished\": os.path.join(dataset_root, \"Finished\"),\n",
                "            \"output_processed\": os.path.join(dataset_root, \"patient_cases_processed.json\"),\n",
                "            \"output_combined\": os.path.join(dataset_root, \"combined_rag_data.json\")\n",
                "        }\n",
                "    else:\n",
                "        print(\"ERROR: Could not find dataset root containing 'medicalKnowledgeBase'.\")\n",
                "        return None\n",
                "\n",
                "paths = get_dataset_paths()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "d9bb41aa",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Reading KB from: c:\\Users\\Muhammad Abu Huraira\\Documents\\Assignments and Submissions\\Semester 7\\NLP\\A04\\NLP_Project04_RAG\\mimic-iv-ext-direct-1.0.0\\medicalKnowledgeBase\\Diagnosis_flowchart\n",
                        "Done! Processed 192 chunks.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import json\n",
                "\n",
                "if paths:\n",
                "    folder_path = paths[\"kb\"]\n",
                "    print(f\"Reading KB from: {folder_path}\")\n",
                "    \n",
                "    all_chunks = []\n",
                "\n",
                "    def Convertorfunction(data, parent_key=\"\"):\n",
                "        items = []\n",
                "        for key, value in data.items():\n",
                "            new_key = f\"{parent_key}/{key}\" if parent_key else key\n",
                "\n",
                "            if isinstance(value, str):\n",
                "                items.append({\"medicalKB\": f\"{new_key}: {value}\"})\n",
                "\n",
                "            elif isinstance(value, list):\n",
                "                items.append({\"medicalKB\": f\"{new_key}: []\"})\n",
                "\n",
                "            elif isinstance(value, dict):\n",
                "                items.extend(Convertorfunction(value, new_key))\n",
                "\n",
                "        return items\n",
                "\n",
                "    if os.path.exists(folder_path):\n",
                "        for file_name in os.listdir(folder_path):\n",
                "            if file_name.endswith(\".json\"):\n",
                "                file_path = os.path.join(folder_path, file_name)\n",
                "\n",
                "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
                "                    data = json.load(f)\n",
                "\n",
                "                all_chunks.extend(Convertorfunction(data))\n",
                "\n",
                "        # --- Save output ---\n",
                "        # Saving to the dataset root for consistency\n",
                "        # output_path = os.path.join(paths[\"root\"], \"ragFile.json\")\n",
                "        # with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
                "        #     json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "        print(f\"Done! Processed {len(all_chunks)} chunks.\")\n",
                "    else:\n",
                "        print(f\"Folder not found: {folder_path}\")\n",
                "else:\n",
                "    print(\"Paths not initialized. Run the first cell.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "ce332e8c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Scanning directory: c:\\Users\\Muhammad Abu Huraira\\Documents\\Assignments and Submissions\\Semester 7\\NLP\\A04\\NLP_Project04_RAG\\mimic-iv-ext-direct-1.0.0\\patient_cases\n",
                        "Processed 511 cases.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import json\n",
                "\n",
                "if paths:\n",
                "    folder_path = paths[\"patient_cases\"]\n",
                "    output_file = paths[\"output_processed\"]\n",
                "\n",
                "    print(f\"Scanning directory: {folder_path}\")\n",
                "\n",
                "    processed_cases = []\n",
                "\n",
                "    def process_patient_case(file_path, file_name, disease_group, specific_disease=None):\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            try:\n",
                "                data = json.load(f)\n",
                "            except json.JSONDecodeError:\n",
                "                print(f\"Error decoding JSON: {file_path}\")\n",
                "                return None\n",
                "\n",
                "        inputs = {}\n",
                "        reasoning = {}\n",
                "        \n",
                "        # Separate inputs and reasoning\n",
                "        for key, value in data.items():\n",
                "            if key.lower().startswith(\"input\"):\n",
                "                inputs[key] = value\n",
                "            else:\n",
                "                reasoning[key] = value\n",
                "                \n",
                "        # Clean inputs\n",
                "        cleaned_inputs = {}\n",
                "        # Standardize to input1..input6\n",
                "        for i in range(1, 7):\n",
                "            key = f\"input{i}\"\n",
                "            val = None\n",
                "            for k in inputs:\n",
                "                if k.lower() == key:\n",
                "                    val = inputs[k]\n",
                "                    break\n",
                "            \n",
                "            if not val or (isinstance(val, str) and val.strip() == \"\"):\n",
                "                val = \"NA\"\n",
                "            \n",
                "            if isinstance(val, str):\n",
                "                cleaned_inputs[key] = val.strip()\n",
                "            else:\n",
                "                cleaned_inputs[key] = val\n",
                "\n",
                "        case_entry = {\n",
                "            \"file_name\": file_name,\n",
                "            \"disease_group\": disease_group,\n",
                "            \"specific_disease\": specific_disease if specific_disease else \"NA\",\n",
                "            \"reasoning\": reasoning,\n",
                "            \"inputs\": cleaned_inputs\n",
                "        }\n",
                "        \n",
                "        return case_entry\n",
                "\n",
                "    # Walk through the directory\n",
                "    if not os.path.exists(folder_path):\n",
                "        print(f\"Error: Directory not found: {folder_path}\")\n",
                "    else:\n",
                "        for root, dirs, files in os.walk(folder_path):\n",
                "            for file in files:\n",
                "                if file.endswith(\".json\"):\n",
                "                    file_path = os.path.join(root, file)\n",
                "                    \n",
                "                    rel_path = os.path.relpath(root, folder_path)\n",
                "                    path_parts = rel_path.split(os.sep)\n",
                "                    \n",
                "                    # path_parts[0] should be 'Finished'\n",
                "                    if len(path_parts) > 1 and path_parts[0] == \"Finished\":\n",
                "                        disease_group = path_parts[1]\n",
                "                        specific_disease = path_parts[2] if len(path_parts) > 2 else None\n",
                "                        \n",
                "                        case_data = process_patient_case(file_path, file, disease_group, specific_disease)\n",
                "                        if case_data:\n",
                "                            processed_cases.append(case_data)\n",
                "\n",
                "        # Save the processed data\n",
                "        # with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
                "        #     json.dump(processed_cases, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "        print(f\"Processed {len(processed_cases)} cases.\")\n",
                "else:\n",
                "    print(\"Paths not initialized.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "185ade7b",
            "metadata": {},
            "source": [
                "# Combined Output: Knowledge Base + Patient Cases\n",
                "This cell creates a single JSON file with:\n",
                "- Upper part: Medical Knowledge Base entries\n",
                "- Lower part: Patient case studies with reasoning flattened to a single string"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "id": "beb4a85b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Knowledge Base Path: c:\\Users\\Muhammad Abu Huraira\\Documents\\Assignments and Submissions\\Semester 7\\NLP\\A04\\NLP_Project04_RAG\\mimic-iv-ext-direct-1.0.0\\medicalKnowledgeBase\\Diagnosis_flowchart\n",
                        "Patient Cases Path: c:\\Users\\Muhammad Abu Huraira\\Documents\\Assignments and Submissions\\Semester 7\\NLP\\A04\\NLP_Project04_RAG\\mimic-iv-ext-direct-1.0.0\\Finished\n",
                        "Processed 96 knowledge base entries\n",
                        "Processed 511 patient cases\n",
                        "\n",
                        "=== Summary ===\n",
                        "Total Knowledge Base entries: 96 (IDs: KB_0001 to KB_0096)\n",
                        "Total Patient Cases: 511\n",
                        "Combined entries: 607\n",
                        "Saved to: c:\\Users\\Muhammad Abu Huraira\\Documents\\Assignments and Submissions\\Semester 7\\NLP\\A04\\NLP_Project04_RAG\\mimic-iv-ext-direct-1.0.0\\combined_rag_data.json\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import json\n",
                "import re\n",
                "\n",
                "if paths:\n",
                "    kb_folder_path = paths[\"kb\"]\n",
                "    # Note: Using 'Finished' folder directly if that's where the cases are for the final combo\n",
                "    # Or we can use the 'patient_cases' folder if that's preferred. \n",
                "    # Based on previous code, it seems to look for 'Finished' inside 'patient_cases' or as a sibling.\n",
                "    # Let's use the 'Finished' path we found.\n",
                "    patient_cases_folder = paths[\"finished\"]\n",
                "    output_file = paths[\"output_combined\"]\n",
                "\n",
                "    print(f\"Knowledge Base Path: {kb_folder_path}\")\n",
                "    print(f\"Patient Cases Path: {patient_cases_folder}\")\n",
                "\n",
                "    # Function to clean anonymization placeholders (___) from text\n",
                "    def clean_text(text):\n",
                "        if not isinstance(text, str):\n",
                "            return text\n",
                "        # Remove standalone ___ (with optional surrounding spaces)\n",
                "        cleaned = re.sub(r'\\s*___\\s*', ' ', text)\n",
                "        # Clean up multiple spaces\n",
                "        cleaned = re.sub(r' +', ' ', cleaned)\n",
                "        # Clean up spaces before punctuation\n",
                "        cleaned = re.sub(r' +([,.:;])', r'\\1', cleaned)\n",
                "        return cleaned.strip()\n",
                "\n",
                "    # Counter for generating unique IDs\n",
                "    kb_id_counter = 0\n",
                "\n",
                "    def Convertorfunction(data, parent_key=\"\"):\n",
                "        global kb_id_counter\n",
                "        items = []\n",
                "        \n",
                "        # Check if this node is a \"concept\" with attributes (dictionary of strings)\n",
                "        all_values_are_strings = True\n",
                "        has_string = False\n",
                "        for k, v in data.items():\n",
                "            if isinstance(v, (dict, list)):\n",
                "                if isinstance(v, list) and len(v) == 0:\n",
                "                    continue\n",
                "                all_values_are_strings = False\n",
                "                break\n",
                "            if isinstance(v, str):\n",
                "                has_string = True\n",
                "                \n",
                "        if all_values_are_strings and has_string:\n",
                "            # It's a concept node. Capture the whole dictionary as the knowledge.\n",
                "            kb_id_counter += 1\n",
                "            knowledge_dict = {}\n",
                "            for k, v in data.items():\n",
                "                if isinstance(v, str):\n",
                "                    knowledge_dict[k] = v\n",
                "            \n",
                "            items.append({\n",
                "                \"id\": f\"KB_{kb_id_counter:04d}\",\n",
                "                \"topic\": parent_key,\n",
                "                \"knowledge\": knowledge_dict\n",
                "            })\n",
                "            return items\n",
                "\n",
                "        # Standard recursion / handling\n",
                "        for key, value in data.items():\n",
                "            new_key = f\"{parent_key}/{key}\" if parent_key else key\n",
                "\n",
                "            if isinstance(value, str):\n",
                "                kb_id_counter += 1\n",
                "                items.append({\n",
                "                    \"id\": f\"KB_{kb_id_counter:04d}\",\n",
                "                    \"topic\": new_key,\n",
                "                    \"knowledge\": value\n",
                "                })\n",
                "            elif isinstance(value, list):\n",
                "                # Skip empty lists to avoid empty knowledge entries\n",
                "                if len(value) > 0:\n",
                "                    kb_id_counter += 1\n",
                "                    items.append({\n",
                "                        \"id\": f\"KB_{kb_id_counter:04d}\",\n",
                "                        \"topic\": new_key,\n",
                "                        \"knowledge\": value\n",
                "                    })\n",
                "            elif isinstance(value, dict):\n",
                "                items.extend(Convertorfunction(value, new_key))\n",
                "                \n",
                "        return items\n",
                "\n",
                "    knowledge_base_entries = []\n",
                "\n",
                "    if os.path.exists(kb_folder_path):\n",
                "        for file_name in os.listdir(kb_folder_path):\n",
                "            if file_name.endswith(\".json\"):\n",
                "                file_path = os.path.join(kb_folder_path, file_name)\n",
                "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
                "                    data = json.load(f)\n",
                "                knowledge_base_entries.extend(Convertorfunction(data))\n",
                "        print(f\"Processed {len(knowledge_base_entries)} knowledge base entries\")\n",
                "    else:\n",
                "        print(f\"Warning: Knowledge base folder not found at {kb_folder_path}\")\n",
                "\n",
                "    def flatten_reasoning(obj, prefix=\"\"):\n",
                "        parts = []\n",
                "        if isinstance(obj, dict):\n",
                "            for key, value in obj.items():\n",
                "                new_prefix = f\"{prefix} -> {key}\" if prefix else key\n",
                "                if isinstance(value, dict) and value:  # Non-empty dict\n",
                "                    parts.append(flatten_reasoning(value, new_prefix))\n",
                "                else:\n",
                "                    parts.append(new_prefix)\n",
                "        return \" | \".join(filter(None, parts))\n",
                "\n",
                "    def process_patient_case(file_path, file_name, disease_group, specific_disease=None):\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            try:\n",
                "                data = json.load(f)\n",
                "            except json.JSONDecodeError:\n",
                "                print(f\"Error decoding JSON: {file_path}\")\n",
                "                return None\n",
                "\n",
                "        inputs = {}\n",
                "        reasoning = {}\n",
                "        for key, value in data.items():\n",
                "            if key.lower().startswith(\"input\"):\n",
                "                inputs[key] = value\n",
                "            else:\n",
                "                reasoning[key] = value\n",
                "\n",
                "        cleaned_inputs = {}\n",
                "        for i in range(1, 7):\n",
                "            key = f\"input{i}\"\n",
                "            val = None\n",
                "            for k in inputs:\n",
                "                if k.lower() == key:\n",
                "                    val = inputs[k]\n",
                "                    break\n",
                "            \n",
                "            if not val or (isinstance(val, str) and val.strip() == \"\"):\n",
                "                val = \"NA\"\n",
                "            \n",
                "            if isinstance(val, str):\n",
                "                # Clean the ___ placeholders from input text\n",
                "                cleaned_inputs[key] = clean_text(val.strip())\n",
                "            else:\n",
                "                cleaned_inputs[key] = val\n",
                "\n",
                "        reasoning_string = flatten_reasoning(reasoning)\n",
                "        \n",
                "        # Use filename (without .json extension) as the ID\n",
                "        case_id = file_name.replace(\".json\", \"\")\n",
                "        \n",
                "        case_entry = {\n",
                "            \"id\": case_id,\n",
                "            \"patient_case\": {\n",
                "                \"disease_group\": disease_group,\n",
                "                \"specific_disease\": specific_disease if specific_disease else \"NA\",\n",
                "                \"reasoning\": reasoning_string,\n",
                "                \"inputs\": cleaned_inputs\n",
                "            }\n",
                "        }\n",
                "        \n",
                "        return case_entry\n",
                "\n",
                "    patient_cases_entries = []\n",
                "\n",
                "    if os.path.exists(patient_cases_folder):\n",
                "        for root, dirs, files in os.walk(patient_cases_folder):\n",
                "            for file in files:\n",
                "                if file.endswith(\".json\"):\n",
                "                    file_path = os.path.join(root, file)\n",
                "                    \n",
                "                    rel_path = os.path.relpath(root, patient_cases_folder)\n",
                "                    path_parts = rel_path.split(os.sep)\n",
                "                    \n",
                "                    # Get disease group and specific disease from folder structure\n",
                "                    if len(path_parts) >= 1 and path_parts[0] != \".\":\n",
                "                        disease_group = path_parts[0]\n",
                "                        specific_disease = path_parts[1] if len(path_parts) > 1 else None\n",
                "                        \n",
                "                        case_data = process_patient_case(file_path, file, disease_group, specific_disease)\n",
                "                        if case_data:\n",
                "                            patient_cases_entries.append(case_data)\n",
                "        print(f\"Processed {len(patient_cases_entries)} patient cases\")\n",
                "    else:\n",
                "        print(f\"Warning: Patient cases folder not found at {patient_cases_folder}\")\n",
                "\n",
                "    # Combine knowledge base entries (upper part) with patient cases (lower part)\n",
                "    combined_data = knowledge_base_entries + patient_cases_entries\n",
                "\n",
                "    # Save the combined output\n",
                "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
                "        json.dump(combined_data, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "    print(f\"\\n=== Summary ===\")\n",
                "    print(f\"Total Knowledge Base entries: {len(knowledge_base_entries)} (IDs: KB_0001 to KB_{kb_id_counter:04d})\")\n",
                "    print(f\"Total Patient Cases: {len(patient_cases_entries)}\")\n",
                "    print(f\"Combined entries: {len(combined_data)}\")\n",
                "    print(f\"Saved to: {output_file}\")\n",
                "else:\n",
                "    print(\"Paths not initialized.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
